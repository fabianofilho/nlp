{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess our text data \n",
    "to convert it to something useful (i.e. numbers) for the machine learning model. We are going to use the Bag-of-Words (BOW) approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing in two ways\n",
    "- modify the original dataframe TEXT column\n",
    "- preprocess as part of your pipeline so you don’t edit the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    # This function preprocesses the text by filling not a number and replacing new lines ('\\n') and carriage returns ('\\r')\n",
    "    df.TEXT = df.TEXT.fillna(' ')\n",
    "    df.TEXT = df.TEXT.str.replace('\\n',' ')\n",
    "    df.TEXT = df.TEXT.str.replace('\\r',' ')\n",
    "    return df\n",
    "# preprocess the text to deal with known issues\n",
    "df_train = preprocess_text(df_train)\n",
    "df_valid = preprocess_text(df_valid)\n",
    "df_test = preprocess_text(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "word_tokenize('This should be tokenized. 02/02/2018 sentence has stars**')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default shows that some punctuation is separated and that numbers stay in the sentence. We will write our own tokenizer function to\n",
    "* replace punctuation with spaces\n",
    "* replace numbers with spaces\n",
    "* lower case all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def tokenizer_better(text):\n",
    "    # tokenize the text by replacing punctuation and numbers with spaces and lowercase all words\n",
    "    \n",
    "    punc_list = string.punctuation+'0123456789'\n",
    "    t = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "    text = text.lower().translate(t)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert free-text into tokens\n",
    "we need a way to count the tokens for each discharge summary. \n",
    "\n",
    "We will use the built in `CountVectorizer` from scikit-learn package. This vectorizer simply counts how many times each word occurs in the note. \n",
    "\n",
    "There is also a `TfidfVectorizer` which takes into how often words are used across all notes, but for this project let’s use the simpler one (I got similar results with the second one too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = ['Data science is about the data', 'The science is amazing', 'Predictive modeling is part of data science']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit \n",
    "the `CountVectorizer` to learn the words in your data and the transform your data to create counts for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(tokenizer = tokenizer_better)\n",
    "vect.fit(sample_text)\n",
    "# matrix is stored as a sparse matrix (since you have a lot of zeros)\n",
    "X = vect.transform(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `matrix X` will be a sparse matrix, but if you convert it to an array `(X.toarray())`, you will see this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`array([[1, 0, 2, 1, 0, 0, 0, 0, 1, 1],\n",
    "       [0, 1, 0, 1, 0, 0, 0, 0, 1, 1],\n",
    "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int64)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where there are 3 rows (since we have 3 notes) and counts of each word. You can see the column names with `vect.get_feature_names()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use only the `training data` because you don’t want to include any new words that show up in the validation and test sets. \n",
    "\n",
    "There is a hyperparameter called max_features which you can set to constrain how many words are included in the Vectorizer. This will use the top N most frequently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit our vectorizer. This will take a while depending on your computer. \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(max_features = 3000, tokenizer = tokenizer_better)\n",
    "# this could take a while\n",
    "vect.fit(df_train.TEXT.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the most frequently used words and we will see that many of these words might not add any value for our model. \n",
    "\n",
    "These words are called stop words, and we can remove them easily (if we want) with the CountVectorizer. \n",
    "\n",
    "There are lists of common stop words for different NLP corpus, but we will just make up our own based on the image below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = ['the','and','to','of','was','with','a','on',\n",
    "                 'in','for','name','is','patient','s','he',\n",
    "                 'at','as','or','one','she','his','her','am',                 \n",
    "                 'were','you','pt','pm','by','be','had','your',\n",
    "                 'this','date','from','there','an','that','p',\n",
    "                 'are','have','has','h','but','o','namepattern',\n",
    "                 'which','every','also']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to add your own stop words if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(max_features = 3000, \n",
    "                       tokenizer = tokenizer_better, \n",
    "                       stop_words = my_stop_words)\n",
    "# this could take a while\n",
    "vect.fit(df_train.TEXT.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform our notes into numerical matrices \n",
    "At this point, I will only use the training and validation data so I’m not tempted to see how it works on the test data yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = vect.transform(df_train.TEXT.values)\n",
    "X_valid_tf = vect.transform(df_valid.TEXT.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need our output labels as separate variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.OUTPUT_LABEL\n",
    "y_valid = df_valid.OUTPUT_LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
