{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# read the admissions table\n",
    "df_adm = pd.read_csv('NOTEEVENTS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main columns of interest are:\n",
    "- SUBJECT_ID\n",
    "- HADM_ID\n",
    "- CATEGORY: includes ‘Discharge summary’, ‘Echo’, ‘ECG’, ‘Nursing’, ‘Physician ‘, ‘Rehab Services’, ‘Case Management ‘, ‘Respiratory ‘, ‘Nutrition’, ‘General’, ‘Social Work’, ‘Pharmacy’, ‘Consult’, ‘Radiology’, ‘Nursing/other’\n",
    "- TEXT: our clinical notes column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset \n",
    "- 2,083,180 rows\n",
    "- Indicating that there are multiple notes per hospitalization.\n",
    "\n",
    "In the notes, \n",
    "- the dates and PHI (name, doctor, location) have been converted for confidentiality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to discharge summary\n",
    "df_notes_dis_sum = df_notes.loc[df_notes.CATEGORY == 'Discharge summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge \n",
    "The notes on the admissions table \n",
    "\n",
    "We might have the assumption that there is one discharge summary per admission, but we should probably check this. We can check this with an assert statement, which ends up failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_notes_dis_sum_duplicated(['HADM_ID']).sum() == 0, 'Multiple discharge summaries per admission'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Investigate why there are multiple summaries, \n",
    "but for simplicity let’s just use the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_dis_sum_last = (df_notes_dis_sum.groupby(['SUBJECT_ID','HADM_ID']).nth(-1)).reset_index()\n",
    "assert df_notes_dis_sum_last.duplicated(['HADM_ID']).sum() == 0, 'Multiple discharge summaries per admission'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a left merge to account for when notes are missing. \n",
    "\n",
    "There are a lot of cases where you get multiple rows after a merge(although we dealt with it above), so I like to add assert statements after a merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm_notes = pd.merge(df_adm[['SUBJECT_ID','HADM_ID','ADMITTIME','DISCHTIME','DAYS_NEXT_ADMIT','NEXT_ADMITTIME','ADMISSION_TYPE','DEATHTIME']],\n",
    "                        df_notes_dis_sum_last[['SUBJECT_ID','HADM_ID','TEXT']], \n",
    "                        on = ['SUBJECT_ID','HADM_ID'],\n",
    "                        how = 'left')\n",
    "assert len(df_adm) == len(df_adm_notes), 'Number of rows increased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.6 % of the admissions are missing `(df_adm_notes.TEXT.isnull().sum()` / `len(df_adm_notes)`), so I investigated a bit further with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm_notes.groupby('ADMISSION_TYPE').apply(lambda g: g.TEXT.isnull().sum())/df_adm_notes.groupby('ADMISSION_TYPE').size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And discovered that 53% of the NEWBORN admissions were missing discharge summaries vs ~4% for the others. \n",
    "\n",
    "At this point I decided to remove the NEWBORN admissions. Most likely, these missing NEWBORN admissions have their discharge summary stored outside of the MIMIC dataset.\n",
    "\n",
    "For this problem, we are going to classify if a patient will be admitted in the next 30 days. \n",
    "\n",
    "Therefore, we need to create a variable with the output label (1 = readmitted, 0 = not readmitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm_notes_clean['OUTPUT_LABEL'] = (df_adm_notes_clean.DAYS_NEXT_ADMIT < 30).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of positive and negative results in \n",
    "- 3004 positive samples\n",
    "- 48109 negative samples. \n",
    "\n",
    "This indicates that we have an imbalanced dataset, which is a common occurrence in healthcare data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data \n",
    "- training \n",
    "- validation \n",
    "- test sets. \n",
    "\n",
    "For reproducible results, I have made the random_state always 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the samples\n",
    "df_adm_notes_clean = df_adm_notes_clean.sample(n = len(df_adm_notes_clean), random_state = 42)\n",
    "df_adm_notes_clean = df_adm_notes_clean.reset_index(drop = True)\n",
    "\n",
    "# Save 30% of the data as validation and test data \n",
    "df_valid_test=df_adm_notes_clean.sample(frac=0.30,random_state=42)\n",
    "df_test = df_valid_test.sample(frac = 0.5, random_state = 42)\n",
    "df_valid = df_valid_test.drop(df_test.index)\n",
    "\n",
    "# use the rest of the data as training data\n",
    "df_train_all=df_adm_notes_clean.drop(df_valid_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the prevalence is so low, we want to prevent the model from always predicting negative (not re-admitted). To do this, we have a few options to balance the training data\n",
    "- sub-sampling the negatives\n",
    "- over-sampling the positives\n",
    "- create synthetic data (e.g. SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training data into positive and negative\n",
    "rows_pos = df_train_all.OUTPUT_LABEL == 1\n",
    "df_train_pos = df_train_all.loc[rows_pos]\n",
    "df_train_neg = df_train_all.loc[~rows_pos]\n",
    "# merge the balanced data\n",
    "df_train = pd.concat([df_train_pos, df_train_neg.sample(n = len(df_train_pos), random_state = 42)],axis = 0)\n",
    "# shuffle the order of training samples \n",
    "df_train = df_train.sample(n = len(df_train), random_state = 42).reset_index(drop = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
