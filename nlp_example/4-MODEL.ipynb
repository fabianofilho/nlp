{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression(C = 0.0001, penalty = 'l2', random_state = 42)\n",
    "clf.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the probability of readmission for each sample with the fitted model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf\n",
    "y_train_preds = model.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = model.predict_proba(X_valid_tf)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess the quality of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC\n",
    "# acc\n",
    "# recall\n",
    "# f1\n",
    "# precision\n",
    "\n",
    "# specificity\n",
    "# prevalence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made many choices (a few below) which we could change and see if there is an improvement:\n",
    "- should we spend time getting more data?\n",
    "- how to tokenize — should we use stemming?\n",
    "- how to vectorize — should we change the number of words?\n",
    "- how to regularized the logistic regression — should we change C or penalty?\n",
    "- which model to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NLP projects that use BOW+logistic regression,\n",
    "## we can plot the most important words to see if we can gain any insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stop_words = ['should','if','it','been','who','during','x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simple things that we can do is try to see the effect of some of our hyperparameters `(max_features and C)`. \n",
    "\n",
    "We could run a grid search, but since we only have 2 parameters here we could look at them separately and see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_LR = GridSearchCV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could try a few other things\n",
    "- change sub-sample to over-sample\n",
    "- add stemming or lemmatization to the tokenizer\n",
    "- test a few different sci-kit learn models\n",
    "- concatenate all the notes instead of the last discharge summary\n",
    "- try a deep learning method such as LSTMs\n",
    "- review the discharge summaries that you are getting wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalizing and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_not_death = df_adm_notes_clean.DEATHTIME.isnull()\n",
    "df_adm_notes_not_death = df_adm_notes_clean.loc[rows_not_death].copy()\n",
    "df_adm_notes_not_death = df_adm_notes_not_death.sample(n = len(df_adm_notes_not_death), random_state = 42)\n",
    "df_adm_notes_not_death = df_adm_notes_not_death.reset_index(drop = True)\n",
    "\n",
    "# Save 30% of the data as validation and test data \n",
    "df_valid_test=df_adm_notes_not_death.sample(frac=0.30,random_state=42)\n",
    "df_test = df_valid_test.sample(frac = 0.5, random_state = 42)\n",
    "df_valid = df_valid_test.drop(df_test.index)\n",
    "\n",
    "# use the rest of the data as training data\n",
    "df_train_all=df_adm_notes_not_death.drop(df_valid_test.index)\n",
    "assert len(df_adm_notes_not_death) == (len(df_test)+len(df_valid)+len(df_train_all)),'math didnt work'\n",
    "\n",
    "# split the training data into positive and negative\n",
    "rows_pos = df_train_all.OUTPUT_LABEL == 1\n",
    "df_train_pos = df_train_all.loc[rows_pos]\n",
    "df_train_neg = df_train_all.loc[~rows_pos]\n",
    "\n",
    "# merge the balanced data\n",
    "df_train = pd.concat([df_train_pos, df_train_neg.sample(n = len(df_train_pos), random_state = 42)],axis = 0)\n",
    "\n",
    "# shuffle the order of training samples \n",
    "df_train = df_train.sample(n = len(df_train), random_state = 42).reset_index(drop = True)\n",
    "\n",
    "# preprocess the text to deal with known issues\n",
    "df_train = preprocess_text(df_train)\n",
    "df_valid = preprocess_text(df_valid)\n",
    "df_test = preprocess_text(df_test)\n",
    "my_new_stop_words = ['the','and','to','of','was','with','a','on','in','for','name',              'is','patient','s','he','at','as','or','one','she','his','her','am',                 'were','you','pt','pm','by','be','had','your','this','date',                'from','there','an','that','p','are','have','has','h','but','o',                'namepattern','which','every','also','should','if','it','been','who','during', 'x']\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(lowercase = True, max_features = 3000, \n",
    "                       tokenizer = tokenizer_better,\n",
    "                      stop_words = my_new_stop_words)\n",
    "\n",
    "# fit the vectorizer\n",
    "vect.fit(df_train.TEXT.values)\n",
    "X_train_tf = vect.transform(df_train.TEXT.values)\n",
    "X_valid_tf = vect.transform(df_valid.TEXT.values)\n",
    "X_test_tf = vect.transform(df_test.TEXT.values)\n",
    "y_train = df_train.OUTPUT_LABEL\n",
    "y_valid = df_valid.OUTPUT_LABEL\n",
    "y_test = df_test.OUTPUT_LABEL\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression(C = 0.0001, penalty = 'l2', random_state = 42)\n",
    "clf.fit(X_train_tf, y_train)\n",
    "model = clf\n",
    "y_train_preds = model.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = model.predict_proba(X_valid_tf)[:,1]\n",
    "y_test_preds = model.predict_proba(X_test_tf)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
